---
title: Week 6 - Simple Bootstrapping

category: Weekly Report
order: 1
---

<!--
1. Bootstrapping Overview
2. Bootstrapping performance
 -->

The target of this week is to 1) introduce the idea of bootstrapping; 2) apply bootstrapping on a simple classifier, i.e. Logistic Regression; 3) evaluate the effectiveness of bootstrapping in the learning steps.


# Overview of Bootstrapping

The supervised machine learning needs a lot of labeled data, while it is difficult to achieve if we start from some snippet in reality. Since we have recognized some high-precision seed pairs of causality relations. It is enough to bootstrap a classifier

In the task of semi-supervised relation extraction, bootstrapping proceeds by considering the entities in the seed pairs, tagging those sentences that contain both entities and generalizing the context around the entities to learn new patterns [<sup>1</sup>](#refer-anchor-1).

However, before we go deep into the discussed step, it is really interesting to check whether the recursively addition of positive examples could result in better performance of a classifier. It will be the focus of this week's blog.


# Bootstrapping Application
## Data processing procedures
At first, it is necessary to acknowledge the data processing procedures of bootstrapping. In our experiment, for the same dataset, we apply bootstrapping with a classifier for *20 rounds*. For each round, the dataset splits into 10 consecutive folds (with shuffling), where **two folds** is used as *test set* and the **eight remaining folds** forms the *train set*. Thus each round triggers 10 repetitions with different splitting ways.

## Learning rate
As we discussed, for each time we got a trained classifier in one round, we expect to include the predicating positive examples in next round. To do so, first we need to set a *learning rate*, also a threshold, to examine which examples are positive (exceeding learning rate) and negative (lower than learning rate). In our experiment, we set the learning rate as 0.9 and 0.8 as comparison.



# Bootstrapping Performance


<!-- ![the increasing positive examples of LR classifier (5 raw)]() -->

![test1](https://github.com/zoeNantes/GSoc2021-DBpedia-NeuralExtraction/blob/gh-page/images/boot_lr.png)

![test2](https://zoenantes.github.io/GSoc2021-DBpedia-NeuralExtraction/images/boot_lr.png)

![test3](./GSoc2021-DBpedia-NeuralExtraction/boot_lr.png)


*Fig 1*

Here in Figure 1, the X-axis presents the different round of bootstrapping and the Y-axis indicates the increased (delta) number of positive examples after this round.

We see that the *learning rate - 0.8* provides the overwhelming positive examples in the first 10 rounds, then it stays at 0; the *learning rate - 0.9* generates a smaller number of positive examples and it ends at 0 after the 4th round.


<!-- ![The figure of bootstraping performance (4 method2)]() -->
<img src="/images/boot_lr.png">


<!-- (https://rawgithub.com/zoeNantes/GSoc2021-DBpedia-NeuralExtraction/blob/gh-page/_docs/weekly_report/boot_lr.png) -->


<!-- https://zoenantes.github.io/GSoc2021-DBpedia-NeuralExtraction/_docs/weekly_report/boot_lr.png) -->
*Fig2*




Here in Figure 2, we applied 4 metrics for each iteration, where each iteration in the X-axis is also namely as each *round* in this blog. In the Y-axis, we provide the averaged value from 10 repetitions. It is noteworthy that we provide the same metric in two conditions: with *learning rate - 0.9* and with *learning rate - 0.8*.

We observe that the *learning rate - 0.8* achieves higher precision (red line) and higher recall (purple line) than that of *learning rate - 0.9*. Thus we could conclude that lower learning rate of bootstrapping has better performance.


Briefly, the simple bootstrapping methods worked well as expected with *learning rate - 0.8*.







## Reference

<div id="refer-anchor-1"></div>
[1] [Information Extraction. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2020. All
rights reserved. Draft of December 30, 2020.](https://web.stanford.edu/~jurafsky/slp3/17.pdf)
