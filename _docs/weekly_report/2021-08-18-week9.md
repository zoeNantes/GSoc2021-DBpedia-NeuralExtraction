---
title: Week 9 - Standard Bootstrapping

category: Weekly Report
order: 1
---

<!--
1. Bootstrapping Overview
2. Bootstrapping performance
 -->

The target of this week is to 1) apply bootstrapping on classifiers in the formal ways; 2) evaluate the performance of different well-trained classifiers, e.g. LSTM and Logistic Regression.


# Bootstrapping - version 2
Previously, we used bootstrapping on different sections of the same dataset. In this blog, we try to train the classification model on one dataset, i.e. *SemEval Dataset*, and assist with bootstrapping from another dataset, i.e. *WikiPages Dataset*.


## Seed preparation
As we discussed, we tried to extract the causality pairs from SemEval dataset (see the details in this [blog](https://zoenantes.github.io/GSoc2021-DBpedia-NeuralExtraction/weekly_report/2021-06-20-week2/) ). In order to use the complete causality information where the causal pairs occurred, we plan to extract the pairs-occurred sentences and complete with some structured information as the training preparation.













## Wiki data preparation




## Classification models




## Classification models with bootstrapping





## Performance of classifiers




At first, it is necessary to acknowledge the data processing procedures of bootstrapping. In our experiment, for the same dataset, we apply bootstrapping with a classifier for *20 rounds*. For each round, the dataset splits into 10 consecutive folds (with shuffling), where **two folds** is used as *test set* and the **eight remaining folds** forms the *train set*. Thus each round triggers 10 repetitions with different splitting ways.

## Learning rate
As we discussed, for each time we got a trained classifier in one round, we expect to include the predicating positive examples in next round. To do so, first we need to set a *learning rate*, also a threshold, to examine which examples are positive (exceeding learning rate) and negative (lower than learning rate). In our experiment, we set the learning rate as 0.9 and 0.8 as comparison.



# Logistic Regression Performance


![The increasing positive examples of LR classifier](https://zoenantes.github.io/GSoc2021-DBpedia-NeuralExtraction/images/boot_lr_delta.png)
*Fig 1*

Here in Figure 1, the X-axis presents the different round of bootstrapping and the Y-axis indicates the increased (delta) number of positive examples after this round.

We see that the *learning rate - 0.8* provides the overwhelming positive examples in the first 10 rounds, then it stays at 0; the *learning rate - 0.9* generates a smaller number of positive examples and it ends at 0 after the 4th round.


![The figure of bootstraping performance](https://zoenantes.github.io/GSoc2021-DBpedia-NeuralExtraction/images/boot_lr.png)
*Fig2*


Here in Figure 2, we applied 4 metrics for each iteration, where each iteration in the X-axis is also namely as each *round* in this blog. In the Y-axis, we provide the averaged value from 10 repetitions. It is noteworthy that we provide the same metric in two conditions: with *learning rate - 0.9* and with *learning rate - 0.8*.

We observe that the *learning rate - 0.8* achieves higher precision (red line) and higher recall (purple line) than that of *learning rate - 0.9*. Thus we could conclude that lower learning rate of bootstrapping has better performance.


Briefly, the simple bootstrapping methods worked well as expected with *learning rate - 0.8*.
